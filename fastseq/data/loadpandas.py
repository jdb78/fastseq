# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/21_data.loadpandas.ipynb (unless otherwise specified).

__all__ = ['CategoryBlock', 'Tabular', 'CreateItemsAdd', 'TSDataLoader', 'concat_ts_list', 'make_test', 'make_test_pct',
           'TSDataLoaders']

# Cell
from ..core import *
from .external import *
from fastcore.utils import *
from fastcore.imports import *
from fastai2.basics import *
from fastai2.data.transforms import *
from fastai2.tabular.all import *
from .transforms import *

# Cell
import numpy as np
import pandas as pd
from torch.utils.data import Dataset, DataLoader

# Cell
def CategoryBlock(vocab=None, add_na=False):
    "`TransformBlock` for single-label categorical targets"
    return TransformBlock(type_tfms=Categorize(vocab=vocab, add_na=add_na))

# Cell
class Tabular(CollBase, GetAttr, FilteredBase):
    "A `DataFrame` wrapper that knows which cols are cont/cat/ts/y, and returns rows in `__getitem__`"
    _default,with_cont='procs',True
    def __init__(self, df, procs=None, cat_names=None, cont_names=None, ts_names=None, y_names=None, block_y=CategoryBlock, splits=None,
                 do_setup=True, device=None):
        if splits is None: splits=[range_of(df)]
        df = df.iloc[sum(splits, [])].copy()
        self.dataloaders = delegates(self._dl_type.__init__)(self.dataloaders)
        super().__init__(df)

        self.y_names,self.device = L(y_names),device
        if block_y is not None:
            if callable(block_y): block_y = block_y()
            procs = L(procs) + block_y.type_tfms
        self.cat_names,self.cont_names,self.procs = L(cat_names),L(cont_names),Pipeline(procs, as_item=True)
        self.split = len(splits[0])
        if do_setup: self.setup()

    def subset(self, i): return self.new(self.items[slice(0,self.split) if i==0 else slice(self.split,len(self))])
    def copy(self): self.items = self.items.copy(); return self
    def new(self, df): return type(self)(df, do_setup=False, block_y=None, **attrdict(self, 'procs','cat_names','cont_names','y_names', 'device'))
    def show(self, max_n=10, **kwargs): display_df(self.all_cols[:max_n])
    def setup(self): self.procs.setup(self)
    def process(self): self.procs(self)
    def loc(self): return self.items.loc
    def iloc(self): return _TabIloc(self)
    def targ(self): return self.items[self.y_names]
    def all_col_names (self): return self.cat_names + self.cont_names + self.y_names
    def n_subsets(self): return 2
    def new_empty(self): return self.new(pd.DataFrame({}, columns=self.items.columns))
    def to_device(self, d=None):
        self.device = d
        return self

properties(Tabular,'loc','iloc','targ','all_col_names','n_subsets')

# Cell
from fastai2.data.all import *

class CreateItemsAdd(Transform):
    def __init__(self, addition=None):
        store_attr(self,'addition')

    def setup(self, *args, **kwargs):
        self.add = self.addition

    def encodes(self, o):
        return o + self.add

    def decodes(self, o):
        return o - self.add


# Cell
# TODO maybe incl. start where the last one ended and therefor keep hidden state
@delegates(TfmdDL.__init__)
class TSDataLoader(TfmdDL):
    def __init__(self, dataset, bs=64, norm=True,
                 num_workers=0, create_item=None, **kwargs):
        res = super().__init__(dataset=self.dataset, bs=bs, num_workers=num_workers **kwargs)

        self.dataset = L(dataset).map(tensor)
        self.norm_items(dataset, norm)
        n = self.make_ids()
        print(self.dataset, horizon, lookback, step)
        after_batch = ifnone(after_batch, noop)
        after_item = ifnone(after_item, noop)
        self.n = n

    def norm_items(self, dataset, norm):
        dataset = dataset.map(tensor)
        r = L()
        for i,ts in enumerate(dataset):
            ts = ts.float()
            if norm:
                ts = (ts - torch.mean(ts.float(), -1, keepdim = True))/(torch.std(ts.float(), -1, keepdim = True)+1e-8)
            r.append(ts)
        return r

    @delegates(DataLoader.new)
    def new(self, dataset=None, cls=None, **kwargs):
        res = super().new(dataset, cls, do_setup=False, **kwargs)
        res.create_item.setup(dataset)
        if not hasattr(self, '_n_inp') or not hasattr(self, '_types'):
            try:
                self._one_pass()
                res._n_inp,res._types = self._n_inp,self._types
            except: print("Could not do one pass in your dataloader, there is something wrong in it")
        return res

    def make_ids(self):
        # Slice each time series into examples, assigning IDs to each
        last_id = 0
        n_dropped = 0
        n_needs_padding = 0
        self._ids = {}
        print(self.dataset,self.horizon,self.lookback,self.step)
        for i, ts in enumerate(self.dataset):
            if isinstance(ts,tuple):
                ts = ts[0] # no idea why they become tuples
            num_examples = (ts.shape[-1] - self.lookback - self.horizon + self.step) // self.step
            # Time series shorter than the forecast horizon need to be dropped.
            if ts.shape[-1] < self.horizon:
                n_dropped += 1
                continue
            # For short time series zero pad the input
            if ts.shape[-1] < self.lookback + self.horizon:
                n_needs_padding += 1
                num_examples = 1
            for j in range(num_examples):
                self._ids[last_id + j] = (i, j * self.step)
            last_id += num_examples

        # Inform user about time series that were too short
        if n_dropped > 0:
            print("Dropped {}/{} time series due to length.".format(
                    n_dropped, len(self.dataset)))

        # Inform user about time series that were short
        if n_needs_padding > 0:
            print("Need to pad {}/{} time series due to length.".format(
                    n_needs_padding, len(self.dataset)))
        # Store the number of training examples
        return int(self._ids.__len__() )

    def get_id(self,idx):
        # Get time series
        ts_id, lookback_id = self._ids[idx]
        ts = self.dataset[ts_id]
        if isinstance(ts,tuple):
            ts = ts[0] # no idea why they become tuples
        # Prepare input and target. Zero pad if necessary.
        if ts.shape[-1] < self.lookback + self.horizon:
            # If the time series is too short, we zero pad
            x = ts[:, :-self.horizon]
            x = np.pad(
                x,
                pad_width=((0, 0), (self.lookback - x.shape[-1], 0)),
                mode='constant',
                constant_values=0
            )
            y = ts[:,-self.horizon:]
        else:
            x = ts[:,lookback_id:lookback_id + self.lookback]
            y = ts[:,lookback_id + self.lookback:lookback_id + self.lookback + self.horizon]
        return x, y

    def shuffle_fn(self, idxs):
        self.dataset.shuffle()
        return idxs

    def create_item(self, idx):
        if idx>=self.n: raise IndexError
        x, y = self.get_id(idx)
        return TSTensorSeq(x),TSTensorSeqy(y, x_len=x.shape[1], m='-*g')


# Cell

from fastai2.vision.data import *

@typedispatch
def show_batch(x: TensorSeq, y, samples, ctxs=None, max_n=10,rows=None, cols=None, figsize=None, **kwargs):
    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), rows=rows, cols=cols, add_vert=1, figsize=figsize)
    ctxs = show_batch[object](x, y, samples=samples, ctxs=ctxs, max_n=max_n, **kwargs)
    return ctxs


# Cell
@typedispatch
def show_results(x: TensorSeq, y, samples, outs, ctxs=None, max_n=9,rows=None, cols=None, figsize=None, **kwargs):
    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), rows=rows, cols=cols, add_vert=1, figsize=figsize)
    for i in range(len(samples[0])):
        ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))]
    for i in range(len(outs[0])):
        ctxs = [TSTensorSeqy(b ,m='*r', label='pred', x_len=x.shape[-1]).show(ctx=c, **kwargs) for b,c,_ in zip(outs.itemgot(i),ctxs,range(max_n))]
    return ctxs

# Cell
def concat_ts_list(train, val):
    items=L()
    assert len(train) == len(val)
    for t, v in zip(train, val):
        items.append(np.concatenate([t,v],1))
    return items

# Cell
def make_test(items:L(), horizon:int, lookback:int, keep_lookback:bool = False):
    """Splits the every ts in `items` based on `horizon + lookback`*, where the last part will go into `val` and the first in `train`.

    *if `keep_lookback`:
        it will only remove `horizon` from `train` otherwise also lookback.
    """
    train, val = L(), L()
    for ts in items:
        val.append(ts[:, -(horizon+lookback):])
        if keep_lookback:
            train.append(ts[:, :-(horizon)])
        else:
            train.append(ts[:, :-(horizon+lookback)])

    return train, val

def make_test_pct(items:L(), pct:float):
    """Splits the every ts in `items` based on `pct`(percentage) of the length of the timeserie, where the last part will go into `val` and the first in `train`.

    """
    train, val = L(), L()
    for ts in items:
        split_idx = int((1-pct)*ts.shape[1])
        train.append(ts[:,:split_idx])
        val.append(ts[:,split_idx:])

    return train, val

# Cell
class TSDataLoaders(DataLoaders):
    @classmethod
    @delegates(TSDataLoader.__init__)
    def from_folder(cls, path, valid_pct=.5, seed=None, horizon=None, lookback=None, step=1, device=None,
                   nrows=None, skiprows=None, **kwargs):
        """Create from M-compition style in `path` with `train`,`test` csv-files.

        The `DataLoader` for the test set will be save as an attribute under `test_dl`
        """
        train, test = get_ts_files(path, nrows=nrows, skiprows=skiprows)
        items = concat_ts_list(train, test)
        horizon = ifnone(horizon, len(test[0]))
        lookback = ifnone(lookback, horizon * 3)
        return cls.from_items(items, horizon, lookback = lookback, path=path, step = step, device = device, **kwargs)


    @classmethod
    @delegates(TSDataLoader.__init__)
    def from_items(cls, items:L, horizon:int, path:Path='.', valid_pct=.5, seed=None, lookback=None, step=1,
                   device=None, **kwargs):
        """Create an list of time series.

        The `DataLoader` for the test set will be save as an attribute under `test_dl`
        """
        lookback = ifnone(lookback, horizon * 4)
        items, test = make_test(items, horizon, lookback, keep_lookback = True)
        train, valid = make_test(items, int(lookback*valid_pct), lookback, keep_lookback = True)

        db = DataLoaders(*[TSDataLoader(items,horizon=horizon, lookback=lookback, step=step, device=device,  **kwargs) for items in [train,valid]])
        db.test = TSDataLoader(test, horizon=horizon, lookback=lookback, step=step, device=device)
        print(f"Train:{db.train.n}; Valid: {db.valid.n}; Test {db.test.n}")
#         TODO add with test_dl, currently give buges
        return db