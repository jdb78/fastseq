# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/11_model.conv.ipynb (unless otherwise specified).

__all__ = ['emb_sz_rule', 'get_emb_sz', 'SeqTabConv', 'dct']

# Cell
from ..core import *
from ..data.external import *
from ..data.load import *
from ..data.core import *
from ..data.procs import *
from fastcore.all import *
from fastcore.imports import *
from fastai2.basics import *
from fastai2.data.transforms import *
from fastai2.tabular.core import *
from fastai2.tabular.model import *
from fastai2.torch_basics import *
from fastai2.callback.all import *
from ..metrics import *

# Cell
def emb_sz_rule(n_cat):
    "Rule of thumb to pick embedding size corresponding to `n_cat`"
    return min(600, round(1.6 * n_cat**0.56))

def _one_emb_sz(classes, n, sz_dict=None):
    "Pick an embedding size for `n` depending on `classes` if not given in `sz_dict`."
    sz_dict = ifnone(sz_dict, {})
    n_cat = len(classes[n])
    sz = sz_dict.get(n, int(emb_sz_rule(n_cat)))  # rule of thumb
    return n_cat,sz

def get_emb_sz(to, typ, sz_dict=None):
    return [_one_emb_sz(to.meta['classes'], col, sz_dict) for col in to.meta['col_names'][typ]]

# Cell
class SeqTabConv(Module):
    """Basic model for sequential data."""
    def __init__(self, horizon, lookback, emb_szs = None, emb_szs_ts = None, ts_con_chn = None, con_chn = None,
                 layers = [32, 32], y_range=[-5,10]):
        self.horizon, self.lookback = horizon, lookback
        if emb_szs is not None:
            self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])
            self.n_emb = sum(e.embedding_dim for e in self.embeds)
        else:
            self.n_emb=0

        if emb_szs is not None:
            self.embeds_ts = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs_ts])
            self.n_emb_ts = sum(e.embedding_dim for e in self.embeds_ts)
        else:
            self.n_emb_ts=0
        self.conv1 = ConvLayer(315, 64, ks = 1, ndim=1)
        self.conv2 = ConvLayer(64, 1, ks = 1, ndim=1)
        self.scale = SigmoidRange(*y_range)

    def forward(self, x, ts_con, ts_cat, cat, con):
        x1 = torch.cat([x, x[:,0,-28:][:,None,:]],2)
        x2 = torch.cat([x, x[:,0,:28][:,None,:]],2)
        x = torch.cat([x1,x2],1)
        if self.n_emb != 0:
            cat = [e(cat[:,i]) for i,e in enumerate(self.embeds)]
            cat = torch.cat(cat, 1)
        if self.n_emb_ts != 0:
            ts_cat = [e(ts_cat[:,i]) for i,e in enumerate(self.embeds_ts)]
            ts_cat = torch.cat(ts_cat, 2).transpose(1,2)
        cat = cat[:,:,None] * torch.ones(ts_con.shape[0], self.n_emb, ts_con.shape[-1]).to(default_device())
        o = torch.ones(ts_con.shape[0], con.shape[1], ts_con.shape[-1]).to(default_device())
        con = con[:,:,None] * o
        ts = torch.cat([x, ts_con, ts_cat, cat, con.float()], 1)
        r = self.conv1(ts)
        r = self.conv2(r)
        return r

# Cell
dct = make_submision_file(learn)
dct['HOBBIES_1_028_CA_1_validation']