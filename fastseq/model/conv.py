# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/12_model.rnn.ipynb (unless otherwise specified).

__all__ = ['pad_start', 'emb_sz_rule', 'get_emb_sz', 'SeqTabConv', 'dct']

# Cell
from ..core import *
from ..data.external import *
from ..data.load import *
from ..data.core import *
from ..data.procs import *
from fastcore.all import *
from fastcore.imports import *
from fastai2.basics import *
from fastai2.data.transforms import *
from fastai2.tabular.core import *
from fastai2.tabular.model import *
from fastai2.torch_basics import *
from fastai2.callback.all import *
from ..metrics import *

# Cell
def pad_start(x, length = 28, fill = 0):
    return torch.cat([torch.ones_like(x)[:,:,:length]*fill, x],-1 )

# Cell
def emb_sz_rule(n_cat):
    "Rule of thumb to pick embedding size corresponding to `n_cat`"
    return min(600, round(1.6 * n_cat**0.56))

def _one_emb_sz(classes, n, sz_dict=None):
    "Pick an embedding size for `n` depending on `classes` if not given in `sz_dict`."
    sz_dict = ifnone(sz_dict, {})
    n_cat = len(classes[n])
    sz = sz_dict.get(n, int(emb_sz_rule(n_cat)))  # rule of thumb
    print(n, n_cat, sz)
    return n_cat,sz

def get_emb_sz(to, typ, sz_dict=None):
    return [_one_emb_sz(to.meta['classes'], col, sz_dict) for col in to.meta['col_names'][typ]]

# Cell
class SeqTabConv(Module):
    """Basic model for sequential data."""
    def __init__(self, horizon, lookback, emb_szs = None, emb_szs_ts = None, ts_con_chn = None, con_chn = None,
                 layers = [32, 32], y_range=[-5,10]):
        self.horizon, self.lookback = horizon, lookback
        if emb_szs is not None:
            self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])
            self.n_emb = sum(e.embedding_dim for e in self.embeds)
        else:
            self.n_emb=0

        if emb_szs is not None:
            self.embeds_ts = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs_ts])
            self.n_emb_ts = sum(e.embedding_dim for e in self.embeds_ts)
        else:
            self.n_emb_ts=0
#         self.conv1 = ConvLayer(286, 500, ks = 3, ndim=1,bn_1st=False)
#         self.conv2 = ConvLayer(500, 100, ks = 3, ndim=1)
#         self.conv3 = ConvLayer(100, 1, ks = 1, ndim=1 )
        self.rnn = nn.LSTM(286,200,2,batch_first=True)
        self.rnn2 = nn.LSTM(200,1,1,batch_first=True)
        self.scale = SigmoidRange(*y_range)

    def forward(self, x, ts_con, ts_cat, cat, con):
        x = pad_start(x, fill = 1)
        if self.n_emb != 0:
            cat = [e(cat[:,i]) for i,e in enumerate(self.embeds)]
            cat = torch.cat(cat, 1)
        if self.n_emb_ts != 0:
            ts_cat = [e(ts_cat[:,i]) for i,e in enumerate(self.embeds_ts)]
            ts_cat = torch.cat(ts_cat, 2).transpose(1,2)
        cat = cat[:,:,None] * torch.ones(ts_con.shape[0], self.n_emb, ts_con.shape[-1]).to(default_device())
        o = torch.ones(ts_con.shape[0], con.shape[1], ts_con.shape[-1]).to(default_device())
        con = con[:,:,None] * o
        ts = torch.cat([x, ts_con, ts_cat, cat, con.float()], 1)
#         print(ts.transpose(1,2).shape)
        a,h = self.rnn(ts.transpose(1,2))
#         print(a.shape)
        r,h = self.rnn2(a)
#         r = self.conv3(r)
#         print(r.shape)
        return r.transpose(1,2)

# Cell
dct = make_submision_file(learn)
dct['HOBBIES_1_028_CA_1_validation']